{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "import os, shutil, time, pdb, random\n",
    "import scipy.stats as stats \n",
    "import scipy\n",
    "\n",
    "from math import pi\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from importlib import reload\n",
    "from models import *\n",
    "from utils import *\n",
    "from runmanager import *\n",
    "from experiment import *\n",
    "from plot_utils import *\n",
    "from preprocessing_utils import *\n",
    "from analysis_seasonal import * \n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rc_file_defaults()\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Detect device.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "start=\"1998-01-01\"\n",
    "end=\"2015-12-31\"\n",
    "\n",
    "# TRAIN_PATH = \"../data/pickle/df_stations_all_nonzero_extended.pkl\"\n",
    "# TEST_PATH = \"../data/pickle/df_stations_val_all_nonzero_extended.pkl\"\n",
    "\n",
    "TRAIN_PATH = '../../data/norris/langtang/observations_with_WRF_norris.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = create_station_dataframe(TRAIN_PATH, start, end, add_yesterday=True, basin_filter = None, filter_incomplete_years = True)\n",
    "\n",
    "data = DataPreprocessing(train_path=TRAIN_PATH, start=start, end=end, \n",
    "                add_yesterday = False, \n",
    "                basin_filter = None, \n",
    "                split_bias_corrected_only = False, \n",
    "                filter_incomplete_years = False, \n",
    "                include_non_bc_stations = True, \n",
    "                split_by = 'station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.split_stations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.st.rename(columns={'Longitude':'X','Latitude':'Y','Elevation':'Z'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.st['doy'] = data.st['Date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.st['doy_sin'] = data.st['doy'].transform(lambda x: np.sin(x))\n",
    "data.st['doy_cos'] = data.st['doy'].transform(lambda x: np.cos(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into held out sets for K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictors = [\n",
    "                'wrf_prcp',\n",
    "                'wrf_prcp_-2','wrf_prcp_-1','wrf_prcp_1','wrf_prcp_2',\n",
    "                'Z',\n",
    "                #'doy',\n",
    "                'doy_sin',\n",
    "                'doy_cos',\n",
    "                'X',\n",
    "                'Y',\n",
    "                'aspect',\n",
    "                'slope',\n",
    "                'year',\n",
    "                'era5_u','era5_u_-1','era5_u_-2','era5_u_1','era5_u_2',\n",
    "                'era5_v','era5_v_-1','era5_v_-2','era5_v_1','era5_v_2',\n",
    "             ]\n",
    "\n",
    "predictors = [ \n",
    "                'doy_sin',\n",
    "                'doy_cos',\n",
    "                'Z',\n",
    "                'X',\n",
    "                'Y',\n",
    "                #'aspect',\n",
    "                #'slope',\n",
    "                'year',\n",
    "                'CWV_norris', \n",
    "                'RH2_norris', 'RH500_norris', \n",
    "                'T2_norris', 'T2max_norris', 'T2min_norris', 'Td2_norris', \n",
    "                'precip_norris', 'rain_norris', \n",
    "                'u10_norris', 'u500_norris', 'v10_norris', 'v500_norris',\n",
    "              ]\n",
    "\n",
    "# predictors.append('obs_yesterday')\n",
    "\n",
    "predictand = ['Prec']\n",
    "\n",
    "data.input_data(predictors, predictand, sort_by_quantile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Run: Train model with different hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = OrderedDict(\n",
    "    lr = [0.005]\n",
    "    ,batch_size = [128]\n",
    "    ,likelihood_fn = ['bgmm','bernoulli_loggaussian'] #, 'bernoulli_loggaussian', 'b2gmm'] #['bernoulli_loggaussian']\n",
    "    ,hidden_channels = [[50]] #[[10],[30],[50],[100],[10,10],[30,30],[50,50],[100,100]]\n",
    "    ,dropout_rate = [0]\n",
    "    ,linear_model = [False, True] #['True','False']\n",
    "    #,k = [0]\n",
    "    ,k = list(range(10))\n",
    ")\n",
    "\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "tags": []
   },
   "outputs": [],
   "source": [
    "st_test, predictions = multirun(data, predictors, params, epochs, split_by='station',\n",
    "                                sequential_samples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in predictions.keys():\n",
    "    for i in range(len(params['k'])):\n",
    "        predictions[run][f'k{i}']['k_fold'] = i\n",
    "        if i == 0:\n",
    "            predictions[run]['k_all'] = predictions[run][f'k{i}']\n",
    "        else:\n",
    "            predictions[run]['k_all'] = predictions[run]['k_all'].append(predictions[run][f'k{i}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of predictions\n",
    "seasons = ['Winter (JFM)', 'Premonsoon (AM)', 'Monsoon (JJAS)','Postmonsoon (OND)']\n",
    "# table_of_predictions(predictions, seasons, sample_cols=['sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in predictions.keys():\n",
    "#     predictions[key]['k_all'].rename(columns={\"sample\": \"sample_0\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions['bgmm_[50]_False']['k_all'].rename(columns={\"wrf_prcp\": \"bannister_wrf_prcp\", \"precip_norris\": \"wrf_prcp\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "sample_cols = [f'sample_{i}' for i in range(n_samples)]\n",
    "add_cols = []\n",
    "\n",
    "columns = ['Prec','wrf_prcp','wrf_bc_prcp','precip_norris'] #+ sample_cols + add_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in predictions.keys(): \n",
    "    for k,v in predictions[p].items():\n",
    "        v['wrf_prcp'] = v['precip_norris'] \n",
    "        v['wrf_bc_prcp'] = v['precip_norris'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_predictions_ks_test(predictions, seasons, columns, sample_cols, add_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_predictions_for_metric(predictions, seasons, columns, n_samples, sample_cols, add_cols, metric = 'smape', prefix='smape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_predictions_for_metric(predictions, seasons, columns, n_samples, sample_cols, add_cols, metric = 'edd', prefix='edd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_predictions_for_metric(predictions, seasons, columns, n_samples, sample_cols, add_cols, metric = 'ae', prefix='ae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_predictions_for_metric(predictions, seasons, columns, n_samples, sample_cols, add_cols, metric = 'se', prefix='se')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('results.csv')\n",
    "\n",
    "b = a.groupby(['k','run']).agg({'valid_loss': 'min', \n",
    "                                 'hidden_channels': 'first', \n",
    "                                 'likelihood_fn': 'first',\n",
    "                                 'lr':'first',\n",
    "                                 'batch_size':'first',\n",
    "                                 'dropout_rate':'first',\n",
    "                                 'linear_model':'first'})\n",
    "\n",
    "c = b.groupby(['run']).agg({'valid_loss': 'mean', \n",
    "                        'hidden_channels': 'first', \n",
    "                        'likelihood_fn': 'first',\n",
    "                        'lr':'first',\n",
    "                        'batch_size':'first',\n",
    "                        'dropout_rate':'first',\n",
    "                         'linear_model':'first'}\n",
    "                   ).sort_values('valid_loss').reset_index()\n",
    "\n",
    "c.groupby(['hidden_channels',\n",
    "           'likelihood_fn',\n",
    "           'lr',\n",
    "           'batch_size',\n",
    "           'dropout_rate',\n",
    "           'linear_model']).agg({'valid_loss': 'mean'}\n",
    "                   ).sort_values('valid_loss').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bcdp]",
   "language": "python",
   "name": "conda-env-bcdp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6f7d0f407510cc81edbbb8fa0d61f79bb70425ade7d5a8e5edb94832bf1e351"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
