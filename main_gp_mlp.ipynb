{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas\n",
    "import os, shutil, time, pdb, random, pickle\n",
    "import scipy.stats as stats \n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from math import pi\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "from importlib import reload\n",
    "from models import *\n",
    "from utils import *\n",
    "from runmanager import *\n",
    "from experiment import *\n",
    "from plot_utils import *\n",
    "from preprocessing_utils import *\n",
    "from elbo import *\n",
    "\n",
    "# from shapely.geometry import box, mapping\n",
    "# import descartes\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# import seaborn as sns\n",
    "# sns.set_theme()\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rc_file_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plum import dispatch\n",
    "from varz.torch import Vars\n",
    "import torch.nn as nn\n",
    "from stheno.torch import B, GP, EQ, Normal, Measure\n",
    "from matrix import Diagonal\n",
    "\n",
    "# Detect device.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gp_mlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Beas and Sutlej shapefiles\n",
    "# beas_shp = '/Users/marron31/Google Drive/PhD/gis/exports/beas_watershed.shp'\n",
    "# sutlej_shp = '/Users/marron31/Google Drive/PhD/gis/exports/sutlej_watershed.shp'\n",
    "\n",
    "# beas = geopandas.read_file(beas_shp)\n",
    "# sutlej = geopandas.read_file(sutlej_shp)\n",
    "\n",
    "# catchments = beas.append(sutlej)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/lab/generic.py:296: DeprecationWarning: The use of `device` to change the active device is deprecated. Please use `on_device` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Let all of Stheno run on that device.\n",
    "B.device(device).__enter__()\n",
    "B.epsilon = 1e-1  # Needs to be relatively high for `float32`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "start=\"2010-01-01\"\n",
    "end=\"2020-12-31\"\n",
    "\n",
    "TRAIN_PATH = \"../../data/norris/enriched_obs/enriched_langtang_obs_norris_ready.pkl\"\n",
    "# TEST_PATH = \"../data/pickle/df_stations_val_all_nonzero_extended.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/marron31/repos/upper-indus-prec-bc/preprocessing_utils.py:278: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[series][df[series] < 0] = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8678"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = DataPreprocessing(TRAIN_PATH, start, end)\n",
    "len(st.st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8543"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = st.st.drop_duplicates()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictant = ['Prec']\n",
    "predictors = [\n",
    "              #'Date',\n",
    "              #'Station',\n",
    "              #'Prec',\n",
    "              #'Corrected Station Name', \n",
    "              'X', 'Y',\n",
    "              #'Altitude (m)', \n",
    "              'Z', \n",
    "              'precip_norris', #'wrf_bc_prcp', \n",
    "              #'elev_hr', \n",
    "              #'aspect','slope', \n",
    "              #'doy', \n",
    "              'doy_sin', 'doy_cos', \n",
    "              #'wrf_prcp_-1', 'wrf_prcp_-2','wrf_prcp_1', 'wrf_prcp_2', \n",
    "              #'Basin', 'lon', 'lat', \n",
    "#               'era5_u', 'era5_v',\n",
    "              #'era5_u_-2', 'era5_u_-1', 'era5_u_1', 'era5_u_2', \n",
    "              #'era5_v_-2', 'era5_v_-1', 'era5_v_1', 'era5_v_2'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_folder = '../../data/norris/enriched_obs/enriched_langtang_obs_norris_ready.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_langtang = ['Tipping Bucket Lama Hotel', \n",
    "                    'Tipping Bucket Langtang',\n",
    "                   'Tipping Bucket Kyanjing', \n",
    "#                    'Tipping Bucket Numthang old',\n",
    "#                    'Tipping Bucket Jathang', \n",
    "#                     'Pluviometer Yala', \n",
    "#                     'AWS Kyangjing',\n",
    "#                    'AWS Yala BC', \n",
    "                    'Tipping Bucket Ganja La 3',\n",
    "                   'Tipping Bucket Ganja La 2',\n",
    "                   'Tipping Bucket Langshisha Glacier (next to Pluviometer)',\n",
    "                   'Tipping Bucket Ganja La 1',\n",
    "                    'Tipping Bucket Langshisha BC',\n",
    "                   'Tipping Bucket Shalbachum',\n",
    "#                    'Pluviometer Langshisha Glacier (off-glacier)',\n",
    "                   'Pluviometer GanjaLa', 'Pluviometer Morimoto']\n",
    "\n",
    "exclusion_list = ['Tipping Bucket Numthang old', 'Tipping Bucket Jathang',\n",
    "                   'Pluviometer Langshisha Glacier (off-glacier)', 'Pluviometer Yala',\n",
    "                   'AWS Kyangjing', 'AWS Yala BC']\n",
    "\n",
    "[s not in exclusion_list for s in stations_langtang]\n",
    "\n",
    "sublist = ['AWS Kyangjing', 'AWS Yala BC', 'Pluviometer Yala']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/marron31/repos/upper-indus-prec-bc/preprocessing_utils.py:278: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[series][df[series] < 0] = 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid comparison between dtype=datetime64[ns] and float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidComparison\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/arrays/datetimelike.py:1008\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_comparison_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidComparison:\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/arrays/datetimelike.py:542\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(other):\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidComparison(other)\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(other) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mInvalidComparison\u001b[0m: 1702735483.943714",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mUpperIndusDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_mean \u001b[38;5;241m=\u001b[39m ds_dataset\u001b[38;5;241m.\u001b[39mmean\n\u001b[1;32m      4\u001b[0m train_var \u001b[38;5;241m=\u001b[39m ds_dataset\u001b[38;5;241m.\u001b[39mvar\n",
      "File \u001b[0;32m~/repos/upper-indus-prec-bc/gp_mlp.py:109\u001b[0m, in \u001b[0;36mUpperIndusDataset.__init__\u001b[0;34m(self, TRAIN_PATH, start, end, predictant, predictors, stations)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, TRAIN_PATH, start, end, predictant, predictors, stations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 109\u001b[0m     st, n, mean, var \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstations \u001b[38;5;241m=\u001b[39m stations\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mst \u001b[38;5;241m=\u001b[39m st\n",
      "File \u001b[0;32m~/repos/upper-indus-prec-bc/gp_mlp.py:127\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(TRAIN_PATH, start, end, predictant, predictors, stations)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(TRAIN_PATH, start, end, predictant, predictors, stations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 127\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mDataPreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     st \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mst\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n\u001b[1;32m    130\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(st)\n",
      "File \u001b[0;32m~/repos/upper-indus-prec-bc/preprocessing_utils.py:60\u001b[0m, in \u001b[0;36mDataPreprocessing.__init__\u001b[0;34m(self, train_path, start, end, add_yesterday, basin_filter, split_bias_corrected_only, filter_incomplete_years, include_non_bc_stations, split_by)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_by \u001b[38;5;241m=\u001b[39m split_by\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Create station dataframe\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mst \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_station_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_yesterday\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasin_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_incomplete_years\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilter_incomplete_years\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstation_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_station_dict()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mst[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStationNum\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mfactorize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mst\u001b[38;5;241m.\u001b[39mStation)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/repos/upper-indus-prec-bc/preprocessing_utils.py:443\u001b[0m, in \u001b[0;36mcreate_station_dataframe\u001b[0;34m(TRAIN_PATH, start, end, add_yesterday, basin_filter, filter_incomplete_years)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_station_dataframe\u001b[39m(TRAIN_PATH: \u001b[38;5;28mstr\u001b[39m, start: \u001b[38;5;28mstr\u001b[39m, end: \u001b[38;5;28mstr\u001b[39m, add_yesterday: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m    441\u001b[0m                              basin_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, filter_incomplete_years \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 443\u001b[0m     st \u001b[38;5;241m=\u001b[39m (\u001b[43mimport_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_drop_dataframe_nan_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrec\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplace_negative_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrec\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_time_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;241m.\u001b[39mpipe(add_year_month_season)\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m.\u001b[39mpipe(calculate_doy_columns)\n\u001b[1;32m    449\u001b[0m         )  \n\u001b[1;32m    451\u001b[0m     \u001b[38;5;66;03m# Add yesterday's observation\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m add_yesterday: st \u001b[38;5;241m=\u001b[39m add_yesterday_observation(st)\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/generic.py:5512\u001b[0m, in \u001b[0;36mNDFrame.pipe\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5454\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   5455\u001b[0m \u001b[38;5;129m@doc\u001b[39m(klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5460\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   5461\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   5462\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5463\u001b[0m \u001b[38;5;124;03m    Apply chainable functions that expect Series or DataFrames.\u001b[39;00m\n\u001b[1;32m   5464\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5510\u001b[0m \u001b[38;5;124;03m    ...  )  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   5511\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/common.py:497\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m(obj, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/upper-indus-prec-bc/preprocessing_utils.py:294\u001b[0m, in \u001b[0;36mclip_time_period\u001b[0;34m(df, start, end, verbose)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_time_period\u001b[39m(df, start, end, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a Dataframe clipped to the time period specified by the start and end dates.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m        df : \u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     df_clip \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbetween\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of clipped dataframe: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_clip)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/series.py:5226\u001b[0m, in \u001b[0;36mSeries.between\u001b[0;34m(self, left, right, inclusive)\u001b[0m\n\u001b[1;32m   5224\u001b[0m         inclusive \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneither\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inclusive \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 5226\u001b[0m     lmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\n\u001b[1;32m   5227\u001b[0m     rmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m right\n\u001b[1;32m   5228\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inclusive \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/ops/common.py:70\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     68\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/arraylike.py:60\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__ge__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/series.py:5623\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5620\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 5623\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:269\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengths must match to compare\u001b[39m\u001b[38;5;124m\"\u001b[39m, lvalues\u001b[38;5;241m.\u001b[39mshape, rvalues\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    265\u001b[0m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    267\u001b[0m ):\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(rvalues) \u001b[38;5;129;01mand\u001b[39;00m isna(rvalues):  \u001b[38;5;66;03m# TODO: but not pd.NA?\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# numpy does not like comparisons vs None\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m operator\u001b[38;5;241m.\u001b[39mne:\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/ops/common.py:70\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     68\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/arraylike.py:60\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__ge__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/arrays/datetimelike.py:1010\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_comparison_value(other)\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidComparison:\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minvalid_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(other, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype):\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# We have to use comp_method_OBJECT_ARRAY instead of numpy\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;66;03m#  comparison otherwise it would fail to raise when\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;66;03m#  comparing tz-aware and tz-naive\u001b[39;00m\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/pandas/core/ops/invalid.py:34\u001b[0m, in \u001b[0;36minvalid_comparison\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     typ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(right)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid comparison between dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid comparison between dtype=datetime64[ns] and float"
     ]
    }
   ],
   "source": [
    "ds_dataset = UpperIndusDataset(TRAIN_PATH, start, end, predictant, predictors, stations=None)\n",
    "\n",
    "train_mean = ds_dataset.mean\n",
    "train_var = ds_dataset.var\n",
    "\n",
    "dataloader = DataLoader(dataset=ds_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "train_dataloader = validation_dataloader = dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 93\n"
     ]
    }
   ],
   "source": [
    "model = MLP(in_channels=5, #len(predictors)+1, \n",
    "            hidden_channels=[10], \n",
    "            likelihood_fn='bgmm', # 'gamma', 'ggmm', bgmm', 'b2gmm', 'b2sgmm'\n",
    "            dropout_rate=0,\n",
    "           )\n",
    "\n",
    "print(f'Number of parameters: {sum(p.numel() for p in model.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       "  (hidden): ModuleList(\n",
       "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
       "  )\n",
       "  (out): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_progress = True\n",
    "plot_x_ind = True\n",
    "validate_flag = False\n",
    "f_marginal_flag = False\n",
    "mc_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr, _ = UpperIndusGridDataset(root_folder, train_mean, train_var).__getitem__(10)\n",
    "\n",
    "# x_min = arr[0,:].min()\n",
    "# x_max = arr[0,:].max() \n",
    "# y_min = arr[1,:].min()\n",
    "# y_max = arr[1,:].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = ds_dataset.st\n",
    "x_ind_stations = st.groupby('Station').mean()[['X','Y']].values\n",
    "x_ind_stations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12065/3299178086.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352660876/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  grid = torch.tensor(grid, dtype=torch.float32).flatten(start_dim=1).permute(1,0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions {'Date'} do not exist. Expected one or more of Frozen({'index': 8543})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m val_kl_batch\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     68\u001b[0m n \u001b[38;5;241m=\u001b[39m train_dataloader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     72\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     73\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/repos/upper-indus-prec-bc/gp_mlp.py:122\u001b[0m, in \u001b[0;36mUpperIndusDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m,idx):\n\u001b[0;32m--> 122\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_array()\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr[\u001b[38;5;241m1\u001b[39m:,:], arr[\u001b[38;5;241m0\u001b[39m,:]\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/xarray/core/dataset.py:2359\u001b[0m, in \u001b[0;36mDataset.isel\u001b[0;34m(self, indexers, drop, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_isel_fancy(indexers, drop\u001b[38;5;241m=\u001b[39mdrop, missing_dims\u001b[38;5;241m=\u001b[39mmissing_dims)\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;66;03m# Much faster algorithm for when all indexers are ints, slices, one-dimensional\u001b[39;00m\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;66;03m# lists, or zero or one-dimensional np.ndarray's\u001b[39;00m\n\u001b[0;32m-> 2359\u001b[0m indexers \u001b[38;5;241m=\u001b[39m \u001b[43mdrop_dims_from_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2361\u001b[0m variables \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2362\u001b[0m dims: Dict[Hashable, Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/xarray/core/utils.py:835\u001b[0m, in \u001b[0;36mdrop_dims_from_indexers\u001b[0;34m(indexers, dims, missing_dims)\u001b[0m\n\u001b[1;32m    833\u001b[0m     invalid \u001b[38;5;241m=\u001b[39m indexers\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(dims)\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m invalid:\n\u001b[0;32m--> 835\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    836\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist. Expected one or more of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    837\u001b[0m         )\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indexers\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m missing_dims \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    842\u001b[0m \n\u001b[1;32m    843\u001b[0m     \u001b[38;5;66;03m# don't modify input\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions {'Date'} do not exist. Expected one or more of Frozen({'index': 8543})"
     ]
    }
   ],
   "source": [
    "# Number of points for inducing points grid\n",
    "x_points = 10\n",
    "y_points = 10\n",
    "\n",
    "#  Grid of equally spaced points covering the spatial extent for which station data is available\n",
    "grid = np.meshgrid(np.linspace(st['X'].min(),st['X'].max(),x_points),np.linspace(st['Y'].min(),st['Y'].max(),y_points))\n",
    "# grid = np.meshgrid(np.linspace(x_min,x_max,x_points),np.linspace(y_min,y_max,y_points))\n",
    "\n",
    "# Make grid into a tensor\n",
    "grid = torch.tensor(grid, dtype=torch.float32).flatten(start_dim=1).permute(1,0)\n",
    "\n",
    "# x_ind = grid.clone().detach().requires_grad_(True)\n",
    "# num_ind_points = x_points * y_points\n",
    "\n",
    "# Inducing points are points in the station\n",
    "x_ind = torch.tensor(x_ind_stations, dtype=torch.float32).detach().requires_grad_(False).to(device)\n",
    "num_ind_points = len(x_ind)\n",
    "\n",
    "n = len(st)\n",
    "\n",
    "train_loss, train_loglik, train_kl = [], [], []\n",
    "val_loss, val_loglik, val_kl = [], [], []\n",
    "test_loss, test_loglik, test_kl = [], [], []\n",
    "\n",
    "train_loss_batch, train_loglik_batch, train_kl_batch = RunningAverage(), RunningAverage(), RunningAverage()\n",
    "val_loss_batch, val_loglik_batch, val_kl_batch = RunningAverage(), RunningAverage(), RunningAverage()\n",
    "\n",
    "# model.train()\n",
    "# prior = Measure()\n",
    "\n",
    "with Measure() as prior:\n",
    "    f1 = GP(EQ().stretch(10))\n",
    "#     f2 = GP(EQ().stretch(2))\n",
    "#     f3 = GP(EQ().stretch(0.5))\n",
    "    f = f1\n",
    "\n",
    "# f = GP(EQ().stretch(0.5)) # f is drawn from a GP\n",
    " \n",
    "q = ApproximatePosterior(num_ind_points) # q is the approximate posterior\n",
    "\n",
    "# optimizer = torch.optim.Adam(list(model.parameters())+list(q.parameters())+[x_ind], lr=10e-3)\n",
    "optimizer = torch.optim.Adam(list(model.parameters())+list(q.parameters()), lr=10e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    \n",
    "    # TRAIN EPOCH\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "#     if plot_x_ind and (e % 10 == 0):\n",
    "#         fig, ax = plt.subplots(figsize=(5,5))\n",
    "#         ax.scatter(x_ind.detach()[:,0].cpu(),\n",
    "#                    x_ind.detach()[:,1].cpu()\n",
    "#                   )\n",
    "#         plt.show()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss_batch.reset()\n",
    "    train_loglik_batch.reset()\n",
    "    train_kl_batch.reset()\n",
    "    val_loss_batch.reset()\n",
    "    val_loglik_batch.reset()\n",
    "    val_kl_batch.reset()\n",
    "\n",
    "    n = train_dataloader.dataset.n\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        elbo, recon, kl, num_points = forward_backward_pass(inputs, labels, n, model, optimizer, q, f, x_ind, \n",
    "                                                            inducing_points=True, backward=True, f_marginal=f_marginal_flag, n_samples=mc_samples)\n",
    "\n",
    "        # Keep track of loss terms\n",
    "        train_loss_batch.update(-elbo.item())\n",
    "        train_loglik_batch.update(-recon.item()/num_points)\n",
    "        train_kl_batch.update(kl.item())\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    if print_progress:\n",
    "        print(f'Average values for training epoch {e}: -elbo: {train_loss_batch.avg:.4f} | kl: {train_kl_batch.avg:.4f} | -log-lik: {train_loglik_batch.avg:.4f} -- time elapsed: {elapsed:.2f}')\n",
    "    \n",
    "    # Add average batch loss terms to lists\n",
    "    train_loss.append(train_loss_batch.avg)\n",
    "    train_loglik.append(float(train_loglik_batch.avg.cpu()))\n",
    "    train_kl.append(train_kl_batch.avg)\n",
    "    \n",
    "    if validate_flag:\n",
    "\n",
    "        # VALIDATION EPOCH\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        n = validation_dataloader.dataset.n\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(validation_dataloader):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                elbo, recon, kl, num_points = forward_backward_pass(inputs, labels, n, model, optimizer, q, f, x_ind, \n",
    "                                                                    inducing_points=True, backward=False, f_marginal=f_marginal_flag, n_samples=mc_samples)\n",
    "\n",
    "                # Keep track of loss terms\n",
    "                val_loss_batch.update(-elbo.item())\n",
    "                val_loglik_batch.update(-recon.item()/num_points)\n",
    "                val_kl_batch.update(kl.item())\n",
    "\n",
    "        elapsed_val = time.time() - start\n",
    "\n",
    "        if print_progress:\n",
    "            print(f'Average values for validation epoch {e}: -elbo: {np.mean(val_loss_batch):.4f} | kl: {np.mean(val_kl_batch):.4f} | -log-lik: {np.mean(val_loglik_batch):.4f} -- time elapsed: {elapsed_val:.2f}')\n",
    "\n",
    "        # Add average batch loss terms to lists\n",
    "        val_loss.append(val_loss_batch.avg)\n",
    "        val_loglik.append(val_loglik_batch.avg)\n",
    "        val_kl.append(val_kl_batch.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vars = [train_loss, train_loglik, train_kl] \n",
    "validation_vars = [val_loss, val_loglik, val_kl]\n",
    "\n",
    "labels = ['-elbo','-loglik','kldiv']\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(20,5))\n",
    "for i,ax in enumerate(axes.flatten()):\n",
    "    ax.plot(train_vars[i], label='train')\n",
    "    ax.plot(validation_vars[i], label='val')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{labels[i]} | best: {np.min(train_vars[i]):.2f}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset=UpperIndusGridDataset(root_folder, train_mean, train_var), batch_size=1, shuffle=True)\n",
    "\n",
    "inducing_points = True\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for index, (inputs, inputs_plot) in enumerate(test_dataloader):\n",
    "        \n",
    "        b = inputs.shape[0]\n",
    "        \n",
    "        if index == 0:\n",
    "        \n",
    "            q.build_normal()\n",
    "\n",
    "            x = inputs[0,:2,:].permute(1,0).float()\n",
    "            \n",
    "            inputs = inputs[:,2:,:]\n",
    "\n",
    "            # Sample z and concatenate to inputs\n",
    "            if inducing_points:\n",
    "                \n",
    "                #f_sample = f_post(x).sample(b).permute(1,0).unsqueeze(1)\n",
    "                \n",
    "                f_post = f | (f(x_ind), q.sample())\n",
    "                \n",
    "                if f_marginal_flag:\n",
    "                    f_sample = Normal(f_post.mean(x),\n",
    "                                      Diagonal(f_post.kernel.elwise(x)[:, 0])\n",
    "                                      ).sample().permute(1,0).unsqueeze(1)\n",
    "                else:\n",
    "                    f_sample = f_post(x).sample().permute(1,0).unsqueeze(1)\n",
    "                \n",
    "                inputs = torch.cat([f_sample, inputs], dim=1)\n",
    "                 \n",
    "            else:\n",
    "                q_sample = q.sample(b).permute(1,0).unsqueeze(1)\n",
    "                \n",
    "                inputs = torch.cat([q_sample, inputs], dim=1)\n",
    "\n",
    "            # Masking for missing data\n",
    "            inputs = inputs.permute(0,2,1)\n",
    " \n",
    "            mask = ~torch.any(inputs.isnan(),dim=2)\n",
    "            k = mask.sum()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs[mask].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['X','Y','Z','wrf_prcp','doy_sin','doy_cos','GPsample','pi','alpha','beta']\n",
    "#col_names = ['X','Y','Z','wrf_prcp','doy_sin','doy_cos','GPsample','alpha','beta']\n",
    "\n",
    "data = np.concatenate([inputs_plot.squeeze().permute(1,0).numpy(),\n",
    "                       inputs[:,:,0].permute(1,0),\n",
    "                       outputs.numpy()], axis=1)\n",
    "\n",
    "df = pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    df, geometry=geopandas.points_from_xy(df.X, df.Y))\n",
    "\n",
    "gdf['uniform'] = gdf.apply(lambda x: np.random.uniform(0,1),axis=1)\n",
    "#gdf['uniform'] = np.random.uniform(0,1)\n",
    "\n",
    "gdf['sample'] = gdf.apply(sample, axis=1)\n",
    "\n",
    "gdf['uniform'] = gdf.apply(lambda x: 0.5, axis=1)\n",
    "gdf['bg_median'] = gdf.apply(sample, axis=1)\n",
    "\n",
    "gdf['g_mean'] = gdf['alpha']/gdf['beta']\n",
    "\n",
    "gdf['p'] = 1 - gdf['pi']\n",
    "\n",
    "x_ind_plot = x_ind.detach().numpy() * np.sqrt(train_var[:2]) + train_mean[:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_stations = st.groupby('Station').mean()[['X','Y']] * np.sqrt(train_var[:2]) + train_mean[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_stations = geopandas.GeoDataFrame(geometry=geopandas.points_from_xy(x_ind_plot[:,0], x_ind_plot[:,1]))\n",
    "\n",
    "gdf_stations['q_sample'] = q.sample().detach().numpy()\n",
    "gdf_stations.plot('q_sample', cmap='viridis')\n",
    "\n",
    "# plt.plot(gdf_stations['geometry'].x, gdf_stations['q_sample'], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['wrf_prcp',\n",
    "             'sample',\n",
    "             'p',\n",
    "             'g_mean',\n",
    "             #'bg_median',\n",
    "             'Z',\n",
    "             'GPsample']\n",
    "\n",
    "labels = ['RCM output (raw)', \n",
    "          'BGMM Sample (mm/day)', \n",
    "          'Probability of rain',\n",
    "          'BGMM Mean (slab component) (mm/day)',\n",
    "          #'BGMM Median (mm/day)',\n",
    "          'Elevation (m.a.s.l.)',\n",
    "          'GP Sample']\n",
    "\n",
    "cmaps = ['terrain',\n",
    "         'terrain',\n",
    "         'terrain',\n",
    "         'terrain',\n",
    "         #'Blues',\n",
    "         'terrain',\n",
    "         'terrain']\n",
    "\n",
    "fig, axes = plt.subplots(3,2,figsize=(15,15))\n",
    "\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    \n",
    "    if idx < len(variables):\n",
    "    \n",
    "    #     gdf.set_crs(catchments.crs)\n",
    "    #     gdf_clip = geopandas.clip(gdf, catchments)\n",
    "    \n",
    "        gdf.plot(variables[idx], ax=ax, legend=True, cmap=cmaps[idx], legend_kwds={'shrink': 0.5})\n",
    "        beas.plot(ax=ax, facecolor='None', edgecolor='black')\n",
    "        #gdf_stations.plot(ax=ax, color='red')\n",
    "        #sutlej.plot(ax=ax, facecolor='None', edgecolor='black')\n",
    "\n",
    "        ax.set_title(labels[idx])\n",
    "        \n",
    "        #if variables[idx]=='GPsample':\n",
    "            #ax.scatter(x_ind_plot[:,0],x_ind_plot[:,1], s=4, c='r')\n",
    "            #ax.scatter(st_stations['X'],st_stations['Y'], s=3, c='w')\n",
    "\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])     \n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bcdp]",
   "language": "python",
   "name": "conda-env-bcdp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
