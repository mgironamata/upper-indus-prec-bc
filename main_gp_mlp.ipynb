{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 27 20:58:35 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A2                      On  |   00000000:98:00.0 Off |                    0 |\n",
      "|  0%   30C    P8              7W /   60W |       0MiB /  15356MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import time, pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import *\n",
    "from runmanager import *\n",
    "from experiment import *\n",
    "from plot_utils import *\n",
    "from preprocessing_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/fdm/fdm.py:38: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n",
      "  coefs = mat.inv()[:, deriv] * np.math.factorial(deriv)\n",
      "/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/fdm/fdm.py:44: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n",
      "  / np.math.factorial(order)\n",
      "/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/fdm/fdm.py:38: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n",
      "  coefs = mat.inv()[:, deriv] * np.math.factorial(deriv)\n",
      "/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/fdm/fdm.py:44: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n",
      "  / np.math.factorial(order)\n"
     ]
    }
   ],
   "source": [
    "from elbo import ApproximatePosterior\n",
    "from gp_mlp import forward_backward_pass, MapDataset, MultipleOptimizer, UpperIndusGridDataset, UpperIndusDataset, prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from stheno.torch import B, GP, EQ, Normal, Measure, Matern12, Matern32, Matern52\n",
    "\n",
    "# Detect device.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let all of Stheno run on that device.\n",
    "B.on_device(device).__enter__();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates\n",
    "start=\"1900-01-01\"\n",
    "end=\"2020-12-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_region = 'alps_occurrence' # 'alps' or 'langtang'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_region == 'alps':\n",
    "    TRAIN_PATH = '../../data/idaweb/observations_enriched_WRF_reduced.pkl'\n",
    "    TEST_PATH = r'/users/marron31/data/idaweb/maps_for_predictions/1991-06-03.csv'\n",
    "\n",
    "if experiment_region == 'langtang':\n",
    "    TRAIN_PATH = '../../data/norris/enriched_obs/enriched_langtang_obs_norris_ready.pkl'\n",
    "\n",
    "if experiment_region == 'alps_2000':\n",
    "    TRAIN_PATH = 'alps_2000.pkl'\n",
    "    TEST_PATH = r'/users/marron31/data/idaweb/maps_for_predictions/1991-06-03.csv'\n",
    "    \n",
    "if experiment_region == 'alps_2000s':    \n",
    "    TRAIN_PATH = 'alps_2000s.pkl'\n",
    "    TEST_PATH = r'/users/marron31/data/idaweb/maps_for_predictions/1991-06-03.csv'\n",
    "    \n",
    "if experiment_region == 'langtang_occurrence':\n",
    "    TRAIN_PATH = '../../data/norris/enriched_obs/enriched_langtang_obs_norris_ready_occurrence.pkl'\n",
    "    \n",
    "if experiment_region == 'alps_occurrence':\n",
    "    TRAIN_PATH = '../../data/idaweb/observations_enriched_WRF_reduced_occurrence.pkl'\n",
    "    TEST_PATH = r'/users/marron31/data/idaweb/maps_for_predictions/1991-06-03.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictant = ['Prec']\n",
    "\n",
    "if 'alps' in experiment_region:\n",
    "    predictors = [\n",
    "                  'X','Y',#'Z', \n",
    "                  'RAIN',\n",
    "                  'doy_sin', 'doy_cos', \n",
    "                  'W500', 'RH2', 'U500', 'V10', \n",
    "                  'RH500', 'U10', 'V500', \n",
    "                  'T2MIN', 'T2', 'T2MAX'\n",
    "                 ]\n",
    "\n",
    "if 'langtang' in experiment_region:\n",
    "    predictors = [\n",
    "                  'X', 'Y', #'Z',\n",
    "                  'precip_norris',\n",
    "                  'doy_sin', 'doy_cos', \n",
    "                  'w500_norris','RH2_norris','u500_norris','v10_norris',\n",
    "                  'RH500_norris','u10_norris','v500_norris',\n",
    "                  'T2min_norris','T2_norris','T2max_norris'\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially used to create split but then manually setting stations because \n",
    "# there is some issue with stations missing after data preparation process\n",
    "# pick 10% random stations for validation, 10% for testing and the rest for training\n",
    "\n",
    "# stations = list(pd.read_pickle(TRAIN_PATH).Station.unique())\n",
    "# random.shuffle(stations)\n",
    "# val_stations = stations[:int(len(stations)*0.1)]\n",
    "# test_stations = stations[int(len(stations)*0.1):int(len(stations)*0.2)]\n",
    "# train_stations = stations[int(len(stations)*0.2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'alps' in experiment_region:\n",
    "    train_stations = ['ABO', 'AHO', 'AIR', 'ALV', 'APT', 'ARO', 'AUB', 'AVA', 'AVB', 'BAC', 'BAL', 'BEA', 'BEC', 'BEGTH', 'BEGWA', 'BEH', 'BEHAK', 'BEKSE', 'BERIF', 'BES', 'BEV', 'BIN', 'BIV', 'BLA', 'BRA', 'BRW', 'BSP', 'BSW', 'CAV', 'CDF', 'CHA', 'CHB', 'CHD', 'CHM', 'CHW', 'CIM', 'COV', 'CTA', 'CTO', 'CUE', 'DAV', 'DIB', 'DIS', 'DLBAL', 'DLFEB', 'DLZUG', 'DMA', 'DOL', 'EIT', 'ENG', 'ERN', 'EVL', 'EVO', 'FIL', 'FIO', 'FIT', 'FLI', 'FRE', 'GEN', 'GOA', 'GRC', 'GRH', 'GSB', 'GSG', 'GST', 'GTT', 'GWA', 'HER', 'HIR', 'HOE', 'INF', 'IYDEO', 'IYDEU', 'IYMMR', 'IYMUW', 'IYPEN', 'IYPFE', 'IYPFI', 'IYPLI', 'IYPRU', 'IYREI', 'IYRID', 'IYROT', 'IYSMG', 'IYSMT', 'IYSUL', 'IYSUM', 'IYSVP', 'IYSWA', 'IYTER', 'IYTOB', 'IYULT', 'IYVAA', 'IYVAL', 'IYWEL', 'IYWOL', 'KLA', 'KRO', 'KSE', 'LBA', 'LEH', 'LEU', 'LOC', 'LOE', 'MAL', 'MAT', 'MAU', 'MLS', 'MOD', 'MSO', 'MST', 'MTE', 'MUE', 'MUS', 'MVE', 'NABCHA', 'NABDAV', 'NAP', 'NEB', 'OBI', 'OBW', 'PDM', 'PIG', 'PIL', 'PLF', 'PON', 'PSB', 'PUD', 'REC', 'ROB', 'ROE', 'ROG', 'RUM', 'SAB', 'SAE', 'SAF', 'SAM', 'SAN', 'SAP', 'SAS', 'SBA', 'SBE', 'SCU', 'SDO', 'SED', 'SEP', 'SGD', 'SIA', 'SIM', 'SLFAM2', 'SLFEM2', 'SLFFIS', 'SLFGL2', 'SLFMEI', 'SLFOBM', 'SLFSA3', 'SLFSC2', 'SLFSC3', 'SLFTU2', 'SLFUR2', 'SLFURS', 'SNE', 'SOG', 'SRL', 'STP', 'SVG', 'SWA', 'TIBED', 'TIBIA', 'TICOL', 'TIFUS', 'TIOLI', 'TST', 'UNS', 'URB', 'VDLSP', 'VDSEP', 'VEL', 'VIO', 'VRI', 'VSANZ', 'VSARO', 'VSBRI', 'VSCHY', 'VSDUR', 'VSEMO', 'VSFIN', 'VSGDX', 'VSJEI', 'VSMAT', 'VSSAB', 'VSSTA', 'VST', 'VSVER', 'WAW', 'WET', 'ZER', 'ZEV', 'ZNZ']\n",
    "    val_stations = ['ANT', 'BEHAB', 'BIO', 'BOS', 'BRL', 'BUF', 'ELO', 'FIE', 'GRY', 'GUE', 'IYHIN', 'IYTAU', 'KAS', 'LAT', 'LBG', 'MEB', 'RIE', 'SLFSH2', 'SLFSP2', 'SMM', 'SMZ', 'TICVM', 'VAB', 'VSMOI', 'VSTSN']\n",
    "    len(train_stations), len(val_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'langtang' in experiment_region:\n",
    "    stations = ['Tipping Bucket Lama Hotel', \n",
    "                  'Tipping Bucket Langtang',\n",
    "                  'Tipping Bucket Kyanjing', \n",
    "#                 'Tipping Bucket Numthang old',\n",
    "#                 'Tipping Bucket Jathang', \n",
    "#                 'Pluviometer Yala', \n",
    "#                 'AWS Kyangjing',\n",
    "#                 'AWS Yala BC', \n",
    "                  'Tipping Bucket Ganja La 3',\n",
    "                  'Tipping Bucket Ganja La 2',\n",
    "                  'Tipping Bucket Langshisha Glacier (next to Pluviometer)',\n",
    "                  'Tipping Bucket Ganja La 1',\n",
    "                  'Tipping Bucket Langshisha BC',\n",
    "                  'Tipping Bucket Shalbachum',\n",
    "#                 'Pluviometer Langshisha Glacier (off-glacier)',\n",
    "                  'Pluviometer GanjaLa', \n",
    "                  'Pluviometer Morimoto'\n",
    "                 ]\n",
    "\n",
    "    train_stations = stations[:7]\n",
    "    val_stations = stations[7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dataset = UpperIndusDataset(TRAIN_PATH, start, end, predictant, predictors, stations=train_stations)\n",
    "ds_dataset_val = UpperIndusDataset(TRAIN_PATH, start, end, predictant, predictors, stations=val_stations)\n",
    "\n",
    "train_mean = ds_dataset.mean\n",
    "train_var = ds_dataset.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=ds_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=ds_dataset_val, batch_size=16*2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_dataset_test = UpperIndusDataset(TRAIN_PATH, start, end, predictant, predictors, stations=test_stations)\n",
    "# ds_dataset_map = UpperIndusDataset(TEST_PATH, start, end, predictant, predictors, stations=test_stations)\n",
    "# dataloader_test = DataLoader(dataset=ds_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "if 'alps' in experiment_region:\n",
    "    ds_dataset_map = MapDataset(TEST_PATH, predictors, train_mean, train_var)\n",
    "    dataloader_map = DataLoader(dataset=ds_dataset_map, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_GP_dims = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_GP_dims = True\n",
    "\n",
    "if drop_GP_dims:\n",
    "    in_channels = len(predictors) + 1 - num_GP_dims\n",
    "else:\n",
    "    in_channels = len(predictors) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'occurrence' in experiment_region:\n",
    "    likelihood_fn='bernoulli'\n",
    "else:\n",
    "    likelihood_fn='bgmm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3351\n"
     ]
    }
   ],
   "source": [
    "# This is the MLP used in the GP-MLP model\n",
    "model = MLP(in_channels=in_channels, \n",
    "            hidden_channels=[50,50], \n",
    "            likelihood_fn=likelihood_fn, # 'gamma', 'ggmm', bgmm', 'b2gmm', 'b2sgmm'\n",
    "            dropout_rate=0,\n",
    "           )\n",
    "\n",
    "print(f'Number of parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = False\n",
    "\n",
    "# This is the MLP used as a baseline (on its own)\n",
    "if baseline:\n",
    "    MLP = MLP(in_channels= len(predictors), \n",
    "                hidden_channels=[50,50], \n",
    "                likelihood_fn=likelihood_fn, # 'gamma', 'ggmm', bgmm', 'b2gmm', 'b2sgmm'\n",
    "                dropout_rate=0,\n",
    "               )\n",
    "\n",
    "    print(f'Number of parameters: {sum(p.numel() for p in MLP.parameters())}')\n",
    "    MLP.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    '''\n",
    "    This function resets model weights randomly following the original initialization method.\n",
    "    Works for simple architectures and commonly used layers.\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            # If the layer has the method `reset_parameters`, it will be called\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_on_wandb = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_on_wandb:\n",
    "    wandb.init(project=f'GP-MLP-{experiment_region}',\n",
    "               name='MLP-50-50'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline:\n",
    "    MLP.apply(reset_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if baseline:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    train_loader = train_dataloader\n",
    "    valid_loader = val_dataloader\n",
    "    # test_loader = dataloader\n",
    "\n",
    "    epochs = 15\n",
    "\n",
    "    mlp_optimizer = torch.optim.Adam(MLP.parameters(), lr=0.005)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        epoch_train_losses, epoch_val_losses = [], []\n",
    "\n",
    "        MLP.train()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "\n",
    "            inputs = inputs.float().to(device) # [batch_size, num_stations, num_predictiors]\n",
    "            labels = labels.float().to(device) # [batch_size, num_stations]\n",
    "\n",
    "            inputs = inputs.permute([0,2,1])\n",
    "            batch_size = inputs.shape[0]\n",
    "\n",
    "            mlp_optimizer.zero_grad()\n",
    "\n",
    "            outputs = MLP(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, labels, inputs, MLP, reduction='sum', device=device)/batch_size \n",
    "\n",
    "            # print('train: ', epoch, i, loss.item())\n",
    "\n",
    "            if loss.item() != 0: \n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                for param in mlp_optimizer.param_groups[0]['params']:\n",
    "                    nn.utils.clip_grad_value_(param, 1) # Bit of regularisation\n",
    "\n",
    "                mlp_optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "            #train_loss, val_loss, test_loss = train_epoch(MLP, optimizer, train_loader, valid_loader, test_loader=test_loader, print_progress=True, epoch=epoch, device=device, permute=True)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(val_dataloader):\n",
    "\n",
    "                inputs = inputs.float().to(device) # [batch_size, num_stations, num_predictiors]\n",
    "                labels = labels.float().to(device) # [batch_size, num_stations]\n",
    "\n",
    "                inputs = inputs.permute([0,2,1])\n",
    "                batch_size = inputs.shape[0]\n",
    "\n",
    "                outputs = MLP(inputs)\n",
    "\n",
    "                loss = loss_fn(outputs, labels, inputs, MLP, reduction='sum', device=device) / batch_size\n",
    "\n",
    "                epoch_val_losses.append(loss.item())\n",
    "\n",
    "        epoch_train_loss = np.mean(epoch_train_losses)/len(train_stations) \n",
    "        epoch_val_loss = np.mean(epoch_val_losses)/len(val_stations) #, np.mean(test_losses)\n",
    "\n",
    "        print(f'epoch : {epoch+1}, train loss : {np.mean(epoch_train_loss):.4f} , valid loss : {np.mean(epoch_val_loss):.4f}')\n",
    "\n",
    "        if log_on_wandb:\n",
    "            wandb.log({\"train_loss\": epoch_train_loss,  \n",
    "                       \"val_loss\": epoch_val_loss,\n",
    "                       \"epoch\" : epoch,\n",
    "                          })\n",
    "\n",
    "\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "    #     test_losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline:\n",
    "    plt.plot(np.array(train_losses), label='train')\n",
    "    plt.plot(np.array(val_losses), label='val')\n",
    "    plt.title(f\"best val loss: {min(val_losses):.4f}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP-MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagonal jitter (depending on dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "# B.epsilon is the default value for the diagonal jitter of the matrix\n",
    "# Needs to be relatively high for `float32`s\n",
    "\n",
    "if 'langtang' in experiment_region:\n",
    "    B.epsilon = 1e-6\n",
    "elif 'alps' in experiment_region:\n",
    "    B.epsilon = 1e-5\n",
    "    \n",
    "print(B.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(reset_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_progress = True\n",
    "validate_flag = True\n",
    "f_marginal_flag = False\n",
    "mc_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'MLP-50-50_matern_ls_ind_2D_lr10-4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_on_wandb = False\n",
    "\n",
    "if log_on_wandb:\n",
    "    wandb.init(project=f'GP-MLP-{experiment_region}',\n",
    "               name=experiment_name,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ds_dataset.st\n",
    "st_val = ds_dataset_val.st\n",
    "\n",
    "coords_train_stations = st.groupby('Station').mean()[predictors[:num_GP_dims]].values\n",
    "coords_val_stations = st_val.groupby('Station').mean()[predictors[:num_GP_dims]].values\n",
    "\n",
    "x_ind_stations = coords_train_stations\n",
    "x_val_statins = coords_val_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inducing points are points in the station locations\n",
    "# x_ind = torch.tensor(x_ind_stations, dtype=torch.float32).detach().requires_grad_(False).to(device)\n",
    "x_ind = torch.nn.Parameter(torch.tensor(x_ind_stations, dtype=torch.float32, requires_grad=True).to(device))\n",
    "num_ind_points = len(x_ind)\n",
    "\n",
    "n = len(st)\n",
    "\n",
    "log_ls = torch.nn.Parameter(torch.tensor(np.log(0.1), dtype=torch.float32, requires_grad=True).to(device))\n",
    "    \n",
    "q = ApproximatePosterior(num_ind_points) # q is the approximate posterior\n",
    "\n",
    "# optimizer = torch.optim.Adam(list(model.parameters())+list(q.parameters()), lr=10e-4)\n",
    "optimizer = torch.optim.Adam(list(model.parameters())+list(q.parameters())+[log_ls], lr=10e-3)\n",
    "# optimizer = torch.optim.Adam(list(model.parameters())+list(q.parameters())+[log_ls]+[x_ind], lr=10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "exp_name = 'matern_ls'\n",
    "\n",
    "# Options for loading an existing model, otherwise start from scratch\n",
    "\n",
    "if load_model:\n",
    "    q.load_state_dict(torch.load(f'q_state_{exp_name}.pth'))\n",
    "    optimizer.load_state_dict(torch.load(f'optimizer_state_{exp_name}.pth'))\n",
    "    model.load_state_dict(torch.load(f'model_state_{exp_name}.pth'))\n",
    "\n",
    "    # Load the data back\n",
    "    with open(f'training_progress_{exp_name}.pkl', 'rb') as file:\n",
    "        data_loaded = pickle.load(file)\n",
    "\n",
    "    e_past = data_loaded[\"last_epoch\"]\n",
    "    train_loss = data_loaded[\"train_loss\"]\n",
    "    train_loglik = data_loaded[\"train_loglik\"]\n",
    "    train_kl = data_loaded[\"train_kl\"]\n",
    "    train_nll = data_loaded[\"train_nll\"]\n",
    "    val_loss = data_loaded[\"val_loss\"]\n",
    "    val_loglik = data_loaded[\"val_loglik\"]\n",
    "    val_kl = data_loaded[\"val_kl\"]\n",
    "    val_nll = data_loaded[\"val_nll\"]\n",
    "\n",
    "else:\n",
    "    e_past = 0\n",
    "    train_loss, train_loglik, train_kl, train_nll = [], [], [], []\n",
    "    val_loss, val_loglik, val_kl, val_nll = [], [], [], []\n",
    "\n",
    "test_loss, test_loglik, test_kl, test_nll  = [], [], [], []\n",
    "\n",
    "train_loss_batch, train_loglik_batch, train_kl_batch, train_nll_batch = RunningAverage(), RunningAverage(), RunningAverage(), RunningAverage()\n",
    "val_loss_batch, val_loglik_batch, val_kl_batch, val_nll_batch = RunningAverage(), RunningAverage(), RunningAverage(),RunningAverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch  1: -elbo: 0.770 | kl: 716.339 | -recon: 0.320 | nll: 0.320 -- time: 64.02\n",
      "Val epoch 1: -elbo: 0.300 | kl: 41.596 | -recon: 0.247 | nll: 0.247 -- time: 52.62\n",
      "GP lengthscale: 0.046\n",
      "---\n",
      "Train epoch  2: -elbo: 0.324 | kl: 34.253 | -recon: 0.302 | nll: 0.302 -- time: 65.00\n",
      "Val epoch 2: -elbo: 0.279 | kl: 30.046 | -recon: 0.240 | nll: 0.240 -- time: 52.88\n",
      "GP lengthscale: 0.047\n",
      "---\n",
      "Train epoch  3: -elbo: 0.319 | kl: 28.991 | -recon: 0.301 | nll: 0.301 -- time: 66.77\n",
      "Val epoch 3: -elbo: 0.274 | kl: 27.445 | -recon: 0.239 | nll: 0.239 -- time: 53.32\n",
      "GP lengthscale: 0.051\n",
      "---\n",
      "Train epoch  4: -elbo: 0.315 | kl: 24.656 | -recon: 0.300 | nll: 0.300 -- time: 66.22\n",
      "Val epoch 4: -elbo: 0.273 | kl: 24.563 | -recon: 0.242 | nll: 0.242 -- time: 53.02\n",
      "GP lengthscale: 0.059\n",
      "---\n",
      "Train epoch  5: -elbo: 0.315 | kl: 24.452 | -recon: 0.299 | nll: 0.299 -- time: 65.73\n",
      "Val epoch 5: -elbo: 0.269 | kl: 24.185 | -recon: 0.238 | nll: 0.238 -- time: 52.81\n",
      "GP lengthscale: 0.081\n",
      "---\n",
      "Train epoch  6: -elbo: 0.313 | kl: 22.732 | -recon: 0.299 | nll: 0.299 -- time: 65.11\n",
      "Val epoch 6: -elbo: 0.266 | kl: 21.474 | -recon: 0.239 | nll: 0.239 -- time: 52.95\n",
      "GP lengthscale: 0.193\n",
      "---\n",
      "Train epoch  7: -elbo: 0.309 | kl: 17.089 | -recon: 0.298 | nll: 0.298 -- time: 79.71\n",
      "Val epoch 7: -elbo: 0.260 | kl: 14.833 | -recon: 0.241 | nll: 0.241 -- time: 73.67\n",
      "GP lengthscale: 1.348\n",
      "---\n",
      "Train epoch  8: -elbo: 0.305 | kl: 11.014 | -recon: 0.298 | nll: 0.298 -- time: 139.83\n",
      "Val epoch 8: -elbo: 0.263 | kl: 19.740 | -recon: 0.238 | nll: 0.238 -- time: 70.37\n",
      "GP lengthscale: 8.846\n",
      "---\n",
      "Train epoch  9: -elbo: 0.302 | kl: 7.704 | -recon: 0.297 | nll: 0.297 -- time: 65.28\n",
      "Val epoch 9: -elbo: 0.283 | kl: 35.587 | -recon: 0.237 | nll: 0.237 -- time: 52.85\n",
      "GP lengthscale: 25.981\n",
      "---\n",
      "Train epoch  10: -elbo: 0.301 | kl: 6.800 | -recon: 0.297 | nll: 0.297 -- time: 65.25\n",
      "Val epoch 10: -elbo: 0.425 | kl: 145.018 | -recon: 0.240 | nll: 0.239 -- time: 53.07\n",
      "GP lengthscale: 46.779\n",
      "---\n",
      "Train epoch  11: -elbo: 0.302 | kl: 8.442 | -recon: 0.297 | nll: 0.297 -- time: 65.52\n",
      "Val epoch 11: -elbo: 0.277 | kl: 32.110 | -recon: 0.236 | nll: 0.236 -- time: 52.83\n",
      "GP lengthscale: 61.247\n",
      "---\n",
      "Train epoch  12: -elbo: 0.302 | kl: 7.380 | -recon: 0.297 | nll: 0.297 -- time: 65.09\n",
      "Val epoch 12: -elbo: 0.460 | kl: 173.103 | -recon: 0.238 | nll: 0.238 -- time: 53.03\n",
      "GP lengthscale: 89.179\n",
      "---\n",
      "Train epoch  13: -elbo: 0.302 | kl: 7.744 | -recon: 0.297 | nll: 0.297 -- time: 66.54\n",
      "Val epoch 13: -elbo: 0.326 | kl: 69.697 | -recon: 0.237 | nll: 0.237 -- time: 52.83\n",
      "GP lengthscale: 97.529\n",
      "---\n",
      "Train epoch  14: -elbo: 0.301 | kl: 6.610 | -recon: 0.296 | nll: 0.296 -- time: 65.23\n",
      "Val epoch 14: -elbo: 0.302 | kl: 48.179 | -recon: 0.240 | nll: 0.240 -- time: 52.72\n",
      "GP lengthscale: 109.946\n",
      "---\n",
      "Train epoch  15: -elbo: 0.300 | kl: 6.416 | -recon: 0.296 | nll: 0.296 -- time: 66.09\n",
      "Val epoch 15: -elbo: 0.669 | kl: 334.140 | -recon: 0.241 | nll: 0.240 -- time: 52.77\n",
      "GP lengthscale: 121.581\n",
      "---\n",
      "Train epoch  16: -elbo: 0.302 | kl: 9.395 | -recon: 0.296 | nll: 0.296 -- time: 65.82\n",
      "Val epoch 16: -elbo: 0.487 | kl: 189.842 | -recon: 0.243 | nll: 0.243 -- time: 52.81\n",
      "GP lengthscale: 113.743\n",
      "---\n",
      "Train epoch  17: -elbo: 0.301 | kl: 7.163 | -recon: 0.296 | nll: 0.296 -- time: 66.60\n",
      "Val epoch 17: -elbo: 0.354 | kl: 91.101 | -recon: 0.237 | nll: 0.237 -- time: 52.73\n",
      "GP lengthscale: 116.234\n",
      "---\n",
      "Train epoch  18: -elbo: 0.300 | kl: 6.539 | -recon: 0.296 | nll: 0.296 -- time: 65.09\n",
      "Val epoch 18: -elbo: 0.303 | kl: 50.451 | -recon: 0.238 | nll: 0.238 -- time: 52.73\n",
      "GP lengthscale: 128.568\n",
      "---\n",
      "Train epoch  19: -elbo: 0.300 | kl: 6.419 | -recon: 0.296 | nll: 0.296 -- time: 64.70\n",
      "Val epoch 19: -elbo: 0.245 | kl: 6.210 | -recon: 0.237 | nll: 0.237 -- time: 52.82\n",
      "GP lengthscale: 107.124\n",
      "---\n",
      "Train epoch  20: -elbo: 0.299 | kl: 5.686 | -recon: 0.296 | nll: 0.296 -- time: 65.32\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "# with Measure() as prior:\n",
    "#     f3 = GP(Matern12().stretch(ls))\n",
    "#     f = f3 #f1 + f2 + f3\n",
    "    \n",
    "for e in range(e_past + 1, num_epochs + 1):\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "#     print(f\"Memory allocated before training epoch: {e}, {torch.cuda.memory_allocated()/(1024**2):.2f}\")\n",
    "#     print(f\"Max memory allocated before training epoch: {e}, {torch.cuda.max_memory_allocated()/(1024**2):.2f}\")\n",
    "    \n",
    "    # TRAIN EPOCH    \n",
    "    model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss_batch.reset()\n",
    "    train_loglik_batch.reset()\n",
    "    train_kl_batch.reset()\n",
    "    train_nll_batch.reset()\n",
    "    val_loss_batch.reset()\n",
    "    val_loglik_batch.reset()\n",
    "    val_kl_batch.reset()\n",
    "    val_nll_batch.reset()\n",
    "\n",
    "    n = train_dataloader.dataset.n\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        inputs = inputs.to(device) # inputs [batch_size, num_predictors, num_stations]\n",
    "        labels = labels.to(device) # labels [batch_size, num_stations]\n",
    "        \n",
    "        with Measure() as prior:\n",
    "            f3 = GP(Matern12().stretch(torch.exp(log_ls)))\n",
    "            f = f3 #f1 + f2 + f3\n",
    "\n",
    "        elbo, recon, kl, num_points, nll = forward_backward_pass(inputs, labels, n, model, optimizer, q, f, x_ind, \n",
    "                                                            inducing_points=True, backward=True, f_marginal=f_marginal_flag, n_samples=mc_samples,\n",
    "                                                            num_GP_dims=num_GP_dims, remove_from_inputs=drop_GP_dims)\n",
    "        \n",
    "#         print(i, elbo.item(), recon.item(), kl.item(), num_points.item(), nll.item())\n",
    "#         print(f'GP lengthscale: {ls.item()}')\n",
    "#         print(torch.stack([torch.isnan(p).sum() for p in model.parameters()]))\n",
    "\n",
    "        # Keep track of loss terms\n",
    "        train_loss_batch.update(-elbo.item())\n",
    "        train_loglik_batch.update(-recon.item()/num_points.item())\n",
    "        train_kl_batch.update(kl.item())\n",
    "        train_nll_batch.update(nll.item()/num_points.item())\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    if print_progress:\n",
    "        print(f'Train epoch  {e}: -elbo: {train_loss_batch.avg:.3f} | kl: {train_kl_batch.avg:.3f} | -recon: {train_loglik_batch.avg:.3f} | nll: {train_nll_batch.avg:.3f} -- time: {elapsed:.2f}')\n",
    "\n",
    "    # Add average batch loss terms to lists\n",
    "    train_loss.append(train_loss_batch.avg)\n",
    "    train_loglik.append(train_loglik_batch.avg)\n",
    "    train_kl.append(train_kl_batch.avg)\n",
    "    train_nll.append(train_nll_batch.avg)\n",
    "    \n",
    "    # VALIDATION EPOCH\n",
    "    validate_flag = True\n",
    "    if validate_flag:\n",
    "        \n",
    "#         print(f\"Memory allocated before validation epoch: {e}, {torch.cuda.memory_allocated()/(1024**2):.2f}\")\n",
    "#         print(f\"Max memory allocated before validation epoch: {e}, {torch.cuda.max_memory_allocated()/(1024**2):.2f}\")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        n = val_dataloader.dataset.n\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(val_dataloader):        \n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                with Measure() as prior:\n",
    "                    f3 = GP(Matern12().stretch(torch.exp(log_ls)))\n",
    "                    f = f3 #f1 + f2 + f3\n",
    "\n",
    "                elbo, recon, kl, num_points, nll = forward_backward_pass(inputs, labels, n, model, optimizer, q, f, x_ind, \n",
    "                                                                    inducing_points=True, backward=False, f_marginal=f_marginal_flag, n_samples=mc_samples*100,\n",
    "                                                                    num_GP_dims=num_GP_dims, remove_from_inputs=drop_GP_dims)\n",
    "\n",
    "                \n",
    "                # Keep track of loss terms\n",
    "                val_loss_batch.update(-elbo.item())\n",
    "                val_loglik_batch.update(-recon.item()/num_points.item())\n",
    "                val_kl_batch.update(kl.item())\n",
    "                val_nll_batch.update(nll.item()/num_points.item())\n",
    "        \n",
    "        elapsed_val = time.time() - start_time\n",
    "\n",
    "        if print_progress:\n",
    "            print(f'Val epoch {e}: -elbo: {val_loss_batch.avg:.3f} | kl: {val_kl_batch.avg:.3f} | -recon: {val_loglik_batch.avg:.3f} | nll: {val_nll_batch.avg:.3f} -- time: {elapsed_val:.2f}')\n",
    "            print(f'GP lengthscale: {torch.exp(log_ls).item():.3f}')\n",
    "            print('---')\n",
    "            \n",
    "        if log_on_wandb:\n",
    "            wandb.log({\"train_nelbo\": train_loss_batch.avg,  \n",
    "                       \"val_nelbo\": val_loss_batch.avg,\n",
    "                       \"train_kl\": train_kl_batch.avg,  \n",
    "                       \"val_kl\": val_kl_batch.avg,\n",
    "                       \"train_nrecon\": train_loglik_batch.avg,  \n",
    "                       \"val_nrecon\": val_loglik_batch.avg,\n",
    "                       \"train_nll\": train_nll_batch.avg,  \n",
    "                       \"val_nll\": val_nll_batch.avg,\n",
    "                       \"epoch\" : e,\n",
    "                       \"ls\" : torch.exp(log_ls).item(),\n",
    "                      })\n",
    "\n",
    "        # Add average batch loss terms to lists\n",
    "        val_loss.append(val_loss_batch.avg)\n",
    "        val_loglik.append(val_loglik_batch.avg)\n",
    "        val_kl.append(val_kl_batch.avg)\n",
    "        val_nll.append(val_nll_batch.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and optimizer state\n",
    "\n",
    "exp_name = experiment_name\n",
    "\n",
    "torch.save(q.state_dict(), f'q_state_{exp_name}.pth')\n",
    "torch.save(model.state_dict(), f'model_state_{exp_name}.pth')\n",
    "torch.save(optimizer.state_dict(), f'optimizer_state_{exp_name}.pth')\n",
    "\n",
    "data_to_save = {\n",
    "    \"last_epoch\": e,\n",
    "    \"train_loss\": train_loss,\n",
    "    \"train_loglik\": train_loglik,\n",
    "    \"train_kl\": train_kl,\n",
    "    \"train_nll\": train_nll,\n",
    "    \"val_loss\": val_loss,\n",
    "    \"val_loglik\": val_loglik,\n",
    "    \"val_kl\": val_kl,\n",
    "    \"val_nll\": val_nll,\n",
    "}\n",
    "\n",
    "# Save the data to a file\n",
    "with open(f'training_progress_{exp_name}.pkl', 'wb') as file:\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(train_vars[i], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(validation_vars[i], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | best: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmin(validation_vars[i])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n\u001b[1;32m     12\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2953\u001b[0m, in \u001b[0;36mmin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2836\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_min_dispatcher)\n\u001b[1;32m   2837\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2838\u001b[0m         where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2839\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2840\u001b[0m \u001b[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[1;32m   2841\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2951\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2954\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/hpcdata/users/marron31/conda-envs/bcdp/lib/python3.9/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_vars = [train_loss, train_loglik, train_kl, train_nll] \n",
    "validation_vars = [val_loss, val_loglik, val_kl, val_nll]\n",
    "\n",
    "labels = ['Negative ELBO','Negative log likelihood term','KL divergence term','NLL']\n",
    "\n",
    "fig, axes = plt.subplots(4,1, figsize=(10,10))\n",
    "for i,ax in enumerate(axes.flatten()):\n",
    "    ax.plot(train_vars[i], label='train')\n",
    "    ax.plot(validation_vars[i], label='val')\n",
    "    ax.set_title(f'{labels[i]} | best: {np.min(validation_vars[i]):.4f}')\n",
    "    ax.set_ylabel(f'{labels[i]}', fontsize=14)\n",
    "    ax.set_xlabel('Epoch', fontsize=14)\n",
    "    ax.set_xticks(np.arange(0, len(train_vars[i])))\n",
    "    ax.legend()\n",
    "#     ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataloader = DataLoader(dataset=UpperIndusGridDataset(root_folder, train_mean, train_var), batch_size=1, shuffle=True)\n",
    "\n",
    "inducing_points = True\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# n = val_dataloader.dataset.n\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i, (inputs, aux_inputs) in enumerate(dataloader_map):\n",
    "        \n",
    "        inputs = inputs.permute(0,2,1).to(device)\n",
    "        og_inputs = inputs.to('cpu')\n",
    "#         pdb.set_trace()\n",
    "        inputs, outputs = forward_backward_pass(inputs, None, n, model, optimizer, q, f, x_ind, num_GP_dims=num_GP_dims,\n",
    "                                                inducing_points=True, backward=False, f_marginal=f_marginal_flag, n_samples=1, \n",
    "                                                test_time=True, remove_from_inputs=drop_GP_dims)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MV Normal Distribution\n",
    "q.build_normal()\n",
    "\n",
    "if 'f_samples' in locals():\n",
    "    del f_samples\n",
    "\n",
    "for i in range(5):\n",
    "    f_post = f | (f(x_ind), q.sample())\n",
    "    x = inputs[:,:,:,1:num_GP_dims+1]\n",
    "    f_sample = f_post(x).sample()\n",
    "    if 'f_samples' not in locals():\n",
    "        f_samples = f_sample\n",
    "    else:\n",
    "        f_samples = torch.cat((f_samples, f_sample), dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_samples = f_samples.permute(2,1,3,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['X','Y','wrf_prcp',\n",
    "#              'doy_sin','doy_cos',\n",
    "             'GPsample','pi',#,'alpha','beta',\n",
    "             'S1','S2','S3','S4','S5']\n",
    "#col_names = ['X','Y','Z','wrf_prcp','doy_sin','doy_cos','GPsample','alpha','beta']\n",
    "\n",
    "data = np.concatenate([og_inputs.squeeze().permute(1,0).cpu().numpy()[:,:3],\n",
    "#                        inputs.squeeze().cpu().numpy()[:,9:10], #* np.sqrt(train_var[9:10]) + train_mean[9:10],\n",
    "                       np.expand_dims(inputs.squeeze().cpu().numpy()[:,0], axis=1),\n",
    "                       np.expand_dims(outputs.squeeze().cpu().numpy(), axis=1), # pi\n",
    "                       f_samples.squeeze().detach().cpu().numpy()] # GP samples\n",
    "                      , axis=1\n",
    "                      )\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "df = pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "df.X = df.X * np.sqrt(train_var[0]) + train_mean[0]\n",
    "df.Y = df.Y * np.sqrt(train_var[1]) + train_mean[1]\n",
    "\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    df, geometry=geopandas.points_from_xy(df.X, df.Y))\n",
    "\n",
    "gdf['uniform'] = gdf.apply(lambda x: np.random.uniform(0,1),axis=1)\n",
    "#gdf['uniform'] = np.random.uniform(0,1)\n",
    "\n",
    "gdf['sample'] = gdf.apply(sample_apply, axis=1, likelihood_fn=model.likelihood)\n",
    "\n",
    "gdf['sample_occurrence'] = gdf['sample'].apply(lambda x: 1 if x > 1 else 0)\n",
    "\n",
    "# gdf['uniform'] = gdf.apply(lambda x: 0.5, axis=1)\n",
    "# gdf['bg_median'] = gdf.apply(sample_apply, axis=1)\n",
    "# gdf['g_mean'] = gdf['alpha']/gdf['beta']\n",
    "\n",
    "gdf['p'] = 1 - gdf['pi']\n",
    "gdf['p_binary'] = gdf['sample'].apply(lambda x: 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_stations = st.groupby('Station').mean()[['X','Y']] * np.sqrt(train_var[:2]) + train_mean[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_stations = geopandas.GeoDataFrame(geometry=geopandas.points_from_xy(x_ind_plot[:,0], x_ind_plot[:,1]))\n",
    "gdf_val_stations = geopandas.GeoDataFrame(geometry=geopandas.points_from_xy(x_val_plot[:,0], x_val_plot[:,1]))\n",
    "\n",
    "plt.figure()\n",
    "gdf_stations['q_sample'] = q.sample().detach().cpu().numpy()\n",
    "plt.scatter(x_ind_plot[:,0],x_ind_plot[:,1], label='training locations')\n",
    "plt.scatter(x_val_plot[:,0],x_val_plot[:,1], label='test locations')\n",
    "# gdf_stations.plot('q_sample', cmap='viridis')\n",
    "# gdf_val_stations.plot()\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(gdf_stations['geometry'].x, gdf_stations['q_sample'], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['wrf_prcp',\n",
    "             'p_binary',\n",
    "#              'g_mean',\n",
    "#              'bg_median',\n",
    "#              'Z',\n",
    "             'sample',\n",
    "             'GPsample',\n",
    "             'S1','S2','S3','S4','S5',\n",
    "            ]\n",
    "\n",
    "labels = ['RCM simulated precip. (mm/day)', \n",
    "          'GP-MLP probability of precip. occurrence',\n",
    "#           '(c) GP-MLP Gamma mean precip. (mm/day)',\n",
    "#           '(d) GP-MLP median precip. (mm/day)',\n",
    "#           'Elevation (m.a.s.l.)',\n",
    "          'Sample from GP-MLP outputs (mm/day)', \n",
    "          'GP sample', 'S1','S2','S3','S4','S5',\n",
    "         ]\n",
    "\n",
    "cmaps = ['terrain',\n",
    "         'viridis',\n",
    "         'terrain',\n",
    "         'terrain',\n",
    "         'terrain',\n",
    "         'terrain',\n",
    "         'terrain',\n",
    "         'terrain',\n",
    "         'terrain',\n",
    "        ]\n",
    "\n",
    "fig, axes = plt.subplots(3,3, figsize=(20,20))\n",
    "\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    \n",
    "    if idx < len(variables):\n",
    "    \n",
    "    #     gdf.set_crs(catchments.crs)\n",
    "    #     gdf_clip = geopandas.clip(gdf, catchments)\n",
    "    \n",
    "        gdf.plot(variables[idx], ax=ax, markersize=5, marker='.', legend=True, cmap=cmaps[idx], legend_kwds={'shrink': 0.6},)\n",
    "#         beas.plot(ax=ax, facecolor='None', edgecolor='black')\n",
    "        #gdf_stations.plot(ax=ax, color='red')\n",
    "        #sutlej.plot(ax=ax, facecolor='None', edgecolor='black')\n",
    "\n",
    "        ax.set_title(labels[idx],fontsize=15)\n",
    "        \n",
    "#         if variables[idx]=='GPsample':\n",
    "#             ax.scatter(x_ind_plot[:,0],x_ind_plot[:,1], s=4, c='r')\n",
    "#             ax.scatter(st_stations['X'],st_stations['Y'], s=3, c='w')\n",
    "\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])     \n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('maps2',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bcdp]",
   "language": "python",
   "name": "conda-env-bcdp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
